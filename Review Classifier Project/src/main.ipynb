{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following libraries before running cells\n",
    "pip install transformers\n",
    "pip install torch\n",
    "pip install datasets\n",
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arfaat\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "from transformers import TrainingArguments, Trainer, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import evaluate\n",
    "import numpy as np;\n",
    "from transformers import DataCollatorWithPadding \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['review', 'sentiment'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = load_dataset(\"csv\", data_files={\"train\": \"IMDB Dataset.csv\"})\n",
    "\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# For binary classification, use only two classes\n",
    "model_path = \"distilbert-base-uncased\"  # Use a smaller model for faster processing\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Update the label mappings for binary classification\n",
    "id2label = {\n",
    "    0: \"negative\",  # Negative class\n",
    "    1: \"positive\"   # Positive class\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    \"negative\": 0,\n",
    "    \"positive\": 1\n",
    "}\n",
    "\n",
    "\n",
    "# Load the model with the updated number of classes (2 for binary classification)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,  # For binary classification, use 2 labels\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all base model parameters except the classification head\n",
    "for name, param in model.distilbert.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# If you want to unfreeze the classification head, you can do that as follows:\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Reviews Length:  25000\n",
      "Negative Reviews Length:  25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40000/40000 [02:09<00:00, 308.27 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:33<00:00, 300.57 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: Dataset({\n",
      "    features: ['review', 'sentiment', 'label', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 40000\n",
      "})\n",
      "test data: Dataset({\n",
      "    features: ['review', 'sentiment', 'label', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocess function to tokenize the text and handle labels\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    # Use the 'review' column for the text input (instead of 'reviews.text')\n",
    "    text = [str(review) for review in examples[\"review\"]]  # Extract review text from the dataset\n",
    "    \n",
    "    # Convert 'sentiment' to binary labels: 1 for 'positive' and 0 for 'negative'\n",
    "    labels = [1 if sentiment == 'positive' else 0 for sentiment in examples[\"sentiment\"]]  # Positive = 1, Negative = 0\n",
    "    \n",
    "    # Tokenize and pad/truncate the text (Customize max_length as needed)\n",
    "    tokenized_output = tokenizer(text, truncation=True, padding=\"max_length\", max_length=128)  # Set max_length to a suitable value\n",
    "    \n",
    "    # Add labels to the tokenized output\n",
    "    tokenized_output[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized_output\n",
    "\n",
    "# Convert the dataset into a pandas DataFrame to facilitate splitting\n",
    "train_data_list = dataset_dict[\"train\"].to_pandas()\n",
    "\n",
    "# Convert 'sentiment' to binary labels: 1 for 'positive', 0 for 'negative'\n",
    "train_data_list['label'] = train_data_list['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Separate the data into 'positive' and 'negative' classes\n",
    "df_positive = train_data_list[train_data_list['label'] == 1]\n",
    "df_negative = train_data_list[train_data_list['label'] == 0]\n",
    "\n",
    "# Print the sizes of each class (No need to sample since the dataset is already balanced)\n",
    "print(\"Positive Reviews Length: \", len(df_positive))\n",
    "print(\"Negative Reviews Length: \", len(df_negative))\n",
    "\n",
    "# Combine the balanced data (no need for resampling)\n",
    "df_balanced = pd.concat([df_positive, df_negative])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Convert back to Hugging Face Dataset format after shuffling\n",
    "dataset_dict[\"train\"] = Dataset.from_pandas(df_balanced)\n",
    "\n",
    "# Perform stratified splitting using sklearn to create train and test splits\n",
    "train_data, test_data = train_test_split(\n",
    "    df_balanced, \n",
    "    test_size=0.2, \n",
    "    stratify=df_balanced['label']  # Ensures balanced class distribution in train and test\n",
    ")\n",
    "\n",
    "# Convert back to Hugging Face Dataset format after splitting\n",
    "train_data = Dataset.from_pandas(train_data)\n",
    "test_data = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Apply preprocessing to both train and test datasets\n",
    "train_data = train_data.map(preprocess_function, batched=True, num_proc=1, fn_kwargs={\"tokenizer\": tokenizer}, keep_in_memory=False)\n",
    "test_data = test_data.map(preprocess_function, batched=True, num_proc=1, fn_kwargs={\"tokenizer\": tokenizer}, keep_in_memory=False)\n",
    "\n",
    "\n",
    "print(\"train data:\", train_data)\n",
    "print(\"test data:\", test_data)\n",
    "# Now, train_data and test_data are ready for use in the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Get the predicted class labels (0 or 1)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Get predicted class probabilities (for AUC calculation)\n",
    "    predicted_probs = predictions[:, 1]  # Probabilities for the positive class (1)\n",
    "\n",
    "    # Calculate AUC using roc_auc_score for binary classification\n",
    "    auc = roc_auc_score(labels, predicted_probs)  # For binary, no need for 'multi_class' option\n",
    "    \n",
    "    # Calculate other metrics\n",
    "    accuracy = accuracy_score(labels, predicted_classes)\n",
    "    precision = precision_score(labels, predicted_classes)\n",
    "    recall = recall_score(labels, predicted_classes)\n",
    "    f1 = f1_score(labels, predicted_classes)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arfaat\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save at the end of each epoch\n",
    "    learning_rate=2e-5,  # Adjust learning rate\n",
    "    per_device_train_batch_size=16,  # Increase batch size for faster processing\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,  # Reduce number of epochs\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_steps=500,  # You may remove this as 'save_strategy' is set to 'epoch'\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # Enable mixed-precision training\n",
    "    gradient_accumulation_steps=2,  # For larger effective batch size\n",
    "    report_to=\"none\"  # Avoid sending reports to external services\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arfaat\\AppData\\Local\\Temp\\ipykernel_3124\\3503835185.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  2%|â–         | 50/2500 [01:46<1:29:02,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6882, 'grad_norm': 1.1521048545837402, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 100/2500 [03:35<1:27:31,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6719, 'grad_norm': 1.4060633182525635, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 150/2500 [05:57<2:52:23,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6592, 'grad_norm': 3.2194876670837402, 'learning_rate': 1.88e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 200/2500 [08:36<1:23:19,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6453, 'grad_norm': 1.2658885717391968, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 250/2500 [10:28<1:22:07,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6352, 'grad_norm': 1.4185434579849243, 'learning_rate': 1.8e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 300/2500 [12:21<1:26:02,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.619, 'grad_norm': 1.912233829498291, 'learning_rate': 1.76e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 350/2500 [14:11<1:23:56,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6025, 'grad_norm': 1.8136245012283325, 'learning_rate': 1.72e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 400/2500 [16:02<1:16:29,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5932, 'grad_norm': 2.1170928478240967, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 450/2500 [17:56<1:16:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5904, 'grad_norm': 1.2311594486236572, 'learning_rate': 1.64e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 500/2500 [19:46<1:11:34,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5745, 'grad_norm': 3.3149280548095703, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 550/2500 [21:36<1:12:04,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5649, 'grad_norm': 1.3637727499008179, 'learning_rate': 1.5600000000000003e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 600/2500 [23:27<1:14:15,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5567, 'grad_norm': 2.6415505409240723, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 650/2500 [25:23<1:07:50,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.555, 'grad_norm': 1.7541048526763916, 'learning_rate': 1.48e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 700/2500 [27:16<1:05:40,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5355, 'grad_norm': 1.7126386165618896, 'learning_rate': 1.4400000000000001e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 750/2500 [29:08<1:03:36,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5329, 'grad_norm': 1.7138564586639404, 'learning_rate': 1.4e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 800/2500 [30:59<1:01:57,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5185, 'grad_norm': 4.2326273918151855, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 850/2500 [32:54<1:02:55,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5309, 'grad_norm': 1.2155306339263916, 'learning_rate': 1.3200000000000002e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 900/2500 [34:47<58:24,  2.19s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5175, 'grad_norm': 1.267491340637207, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 950/2500 [36:39<56:15,  2.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5263, 'grad_norm': 1.3804229497909546, 'learning_rate': 1.2400000000000002e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1000/2500 [38:38<58:13,  2.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5197, 'grad_norm': 1.5906885862350464, 'learning_rate': 1.2e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1050/2500 [40:32<53:37,  2.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5144, 'grad_norm': 2.9044647216796875, 'learning_rate': 1.16e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1100/2500 [42:31<51:10,  2.19s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5026, 'grad_norm': 3.738576889038086, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1150/2500 [44:27<49:17,  2.19s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.501, 'grad_norm': 1.6243363618850708, 'learning_rate': 1.0800000000000002e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1200/2500 [46:19<47:17,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5027, 'grad_norm': 1.264631748199463, 'learning_rate': 1.04e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1250/2500 [48:11<45:25,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4882, 'grad_norm': 2.2259633541107178, 'learning_rate': 1e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1250/2500 [58:14<45:25,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4917764663696289, 'eval_accuracy': 0.7771, 'eval_precision': 0.7817775066097213, 'eval_recall': 0.7688, 'eval_f1': 0.7752344459009781, 'eval_auc': 0.85675492, 'eval_runtime': 602.3487, 'eval_samples_per_second': 16.602, 'eval_steps_per_second': 1.038, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1300/2500 [1:00:03<42:46,  2.14s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4948, 'grad_norm': 2.3550403118133545, 'learning_rate': 9.600000000000001e-06, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1350/2500 [1:01:52<41:47,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5031, 'grad_norm': 2.5809574127197266, 'learning_rate': 9.200000000000002e-06, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1400/2500 [1:03:41<41:52,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5105, 'grad_norm': 1.4291493892669678, 'learning_rate': 8.8e-06, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1450/2500 [1:05:30<37:32,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4944, 'grad_norm': 2.382768392562866, 'learning_rate': 8.400000000000001e-06, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1500/2500 [1:07:20<36:15,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.477, 'grad_norm': 2.2201976776123047, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1550/2500 [1:09:17<34:27,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4861, 'grad_norm': 3.9655847549438477, 'learning_rate': 7.600000000000001e-06, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1600/2500 [1:11:06<32:45,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.48, 'grad_norm': 1.9299707412719727, 'learning_rate': 7.2000000000000005e-06, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1650/2500 [1:12:55<31:41,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4803, 'grad_norm': 2.2534403800964355, 'learning_rate': 6.800000000000001e-06, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1700/2500 [1:14:45<28:38,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4827, 'grad_norm': 2.1702187061309814, 'learning_rate': 6.4000000000000006e-06, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1750/2500 [1:16:35<26:58,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4837, 'grad_norm': 2.771080255508423, 'learning_rate': 6e-06, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1800/2500 [1:18:24<25:29,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4701, 'grad_norm': 1.5763707160949707, 'learning_rate': 5.600000000000001e-06, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1850/2500 [1:20:14<23:33,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4654, 'grad_norm': 1.6097923517227173, 'learning_rate': 5.2e-06, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1900/2500 [1:22:04<21:35,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.474, 'grad_norm': 4.9390764236450195, 'learning_rate': 4.800000000000001e-06, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1950/2500 [1:23:53<19:45,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4773, 'grad_norm': 2.9526755809783936, 'learning_rate': 4.4e-06, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2000/2500 [1:25:45<19:34,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4954, 'grad_norm': 1.4182220697402954, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2050/2500 [1:27:42<18:10,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4617, 'grad_norm': 1.635291576385498, 'learning_rate': 3.6000000000000003e-06, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2100/2500 [1:29:37<15:01,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.476, 'grad_norm': 1.2090306282043457, 'learning_rate': 3.2000000000000003e-06, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2150/2500 [1:31:37<14:53,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4994, 'grad_norm': 1.9815254211425781, 'learning_rate': 2.8000000000000003e-06, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2200/2500 [1:33:27<11:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4647, 'grad_norm': 1.1104131937026978, 'learning_rate': 2.4000000000000003e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2250/2500 [1:35:18<08:58,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4736, 'grad_norm': 2.3386049270629883, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2300/2500 [1:37:09<07:18,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4805, 'grad_norm': 2.841216802597046, 'learning_rate': 1.6000000000000001e-06, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2350/2500 [1:38:58<05:44,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4672, 'grad_norm': 1.5968784093856812, 'learning_rate': 1.2000000000000002e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2400/2500 [1:40:48<03:38,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4725, 'grad_norm': 1.2000746726989746, 'learning_rate': 8.000000000000001e-07, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2450/2500 [1:42:43<01:54,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4696, 'grad_norm': 1.4742268323898315, 'learning_rate': 4.0000000000000003e-07, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [1:44:43<00:00,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4673, 'grad_norm': 1.5131182670593262, 'learning_rate': 0.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [1:56:10<00:00,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4728476107120514, 'eval_accuracy': 0.7832, 'eval_precision': 0.7813990461049285, 'eval_recall': 0.7864, 'eval_f1': 0.7838915470494418, 'eval_auc': 0.8619363600000001, 'eval_runtime': 685.5477, 'eval_samples_per_second': 14.587, 'eval_steps_per_second': 0.912, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [1:56:11<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6971.371, 'train_samples_per_second': 11.476, 'train_steps_per_second': 0.359, 'train_loss': 0.5230634147644043, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.5230634147644043, metrics={'train_runtime': 6971.371, 'train_samples_per_second': 11.476, 'train_steps_per_second': 0.359, 'total_flos': 2649347973120000.0, 'train_loss': 0.5230634147644043, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,  # Your binary classification train dataset\n",
    "    eval_dataset=test_data,    # Your binary classification test dataset\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,  # Ensure you're using the updated metrics function\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results\\\\tokenizer_config.json',\n",
       " 'results\\\\special_tokens_map.json',\n",
       " 'results\\\\vocab.txt',\n",
       " 'results\\\\added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"results\")\n",
    "tokenizer.save_pretrained(\"results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
